{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003a2513",
   "metadata": {},
   "source": [
    "### DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d6ca0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%5B%27NYC+subway%27%2C+%27MTA%27%2C+%27NYC%27%5D+since%3A2012-01-01+until%3A2012-12-31&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%5B%27NYC+subway%27%2C+%27MTA%27%2C+%27NYC%27%5D+since%3A2012-01-01+until%3A2012-12-31&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%5B%27NYC+subway%27%2C+%27MTA%27%2C+%27NYC%27%5D+since%3A2012-01-01+until%3A2012-12-31&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [261]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m until_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(year)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-12-31\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m tweets_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m since:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m until:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muntil_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tweet\u001b[38;5;241m.\u001b[39muser\u001b[38;5;241m.\u001b[39mlocation \u001b[38;5;129;01min\u001b[39;00m locs:\n\u001b[1;32m     22\u001b[0m         tweet_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Location\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39muser\u001b[38;5;241m.\u001b[39mlocation,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Description\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39muser\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Follower Count\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39muser\u001b[38;5;241m.\u001b[39mfollowersCount,\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet Date\u001b[39m\u001b[38;5;124m'\u001b[39m: tweet\u001b[38;5;241m.\u001b[39mdate,}\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:1546\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1544\u001b[0m \t\u001b[38;5;28;01mdel\u001b[39;00m paginationParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_search_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mV2, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor):\n\u001b[1;32m   1547\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_timeline_instructions_to_tweets(obj)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:759\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    758\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 759\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    762\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:725\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[1;32m    724\u001b[0m \tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n\u001b[0;32m--> 725\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/snscrape/base.py:225\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 225\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/snscrape/base.py:221\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    219\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[1;32m    220\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%5B%27NYC+subway%27%2C+%27MTA%27%2C+%27NYC%27%5D+since%3A2012-01-01+until%3A2012-12-31&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "# !pip install snscrape\n",
    "# !pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "# !pip install gender-guesser\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define search query and date range\n",
    "search_words = [\"NYC subway\",\"MTA\",\"NYC\"]\n",
    "locs = ['NY', 'NYC', 'New York', 'Brooklyn', 'New York City']\n",
    "\n",
    "YR = np.linspace(2012,2023,2023-2012+1).astype(int).tolist()\n",
    "\n",
    "for year in YR:\n",
    "    since_date = str(year)+\"-01-01\"\n",
    "    until_date = str(year)+\"-12-31\"\n",
    "\n",
    "    tweets_list = []\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'{search_words} since:{since_date} until:{until_date}',top = True).get_items()):\n",
    "        if tweet.user.location in locs:\n",
    "            tweet_dict = {\n",
    "            'Tweet': tweet.content,\n",
    "            'User Location': tweet.user.location,\n",
    "            'User Description': tweet.user.description,\n",
    "            'User Follower Count': tweet.user.followersCount,\n",
    "            'Tweet Date': tweet.date,}\n",
    "            tweets_list.append(tweet_dict)\n",
    "            print (tweet.content)\n",
    "            del tweet_dict\n",
    "\n",
    "#     data = pd.DataFrame(tweets_list, columns=['Tweet', 'User Location', 'User Description', 'User Follower Count','Tweet Date'])\n",
    "#     data.to_csv('tweets_'+str(year)+'.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a6371b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Input the time range of tweets to scrape\n",
    "since_date = \"2023-01-01\"\n",
    "until_date = \"2023-12-31\"\n",
    "\n",
    "# Input the search words\n",
    "search_words = \"elon musk\"\n",
    "search_query = search_words.replace(\" \", \"%20OR%20\")\n",
    "\n",
    "# Define the URL of the Nitter search results page\n",
    "url = f\"https://nitter.net/search?f=tweets&q={search_query}%20since%3A{since_date}%20until%3A{until_date}\"\n",
    "\n",
    "# Make a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the tweet cards on the page\n",
    "tweet_cards = soup.find_all('div', {'class': 'tweet'})\n",
    "\n",
    "# Extract the text content of each tweet\n",
    "tweets = []\n",
    "for card in tweet_cards:\n",
    "    tweet_text = card.find('div', {'class': 'tweet-content'}).text.strip()\n",
    "    tweets.append(tweet_text)\n",
    "    print(tweet_text)\n",
    "\n",
    "# Print the extracted tweets\n",
    "print(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e164e",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fcaea25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('/Users/abhishek/Desktop/PSU_Courses/CE 597 Public transportation/Project/tweets_2010.csv')\n",
    "# df['Tweet'] = df['Tweet'].astype(str)\n",
    "# df['Tweet'] = df['Tweet'].apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ff420615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import TweetTokenizer\n",
    "# tknzr = TweetTokenizer()\n",
    "# df['Tweet_tokenize'] = df['Tweet'].apply(lambda s: tknzr.tokenize(s))\n",
    "# df[\"Tweet_tokenize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3f9a883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# df[\"Tweet_tokenize\"] = df[\"Tweet_tokenize\"].apply(lambda l: [s.translate(str.maketrans('','',string.punctuation)) for s in l])\n",
    "# df[\"Tweet_tokenize\"] = df[\"Tweet_tokenize\"].apply(lambda l: [s for s in l if s.isalpha()])\n",
    "# df[\"Tweet_tokenize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cdfa46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# # nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english')) #\n",
    "# df[\"Tweet_tokenize\"] = df[\"Tweet_tokenize\"].apply(lambda l: [s for s in l if s not in stop_words])\n",
    "# df[\"Tweet_tokenize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bdd2380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "# stemmer = PorterStemmer()\n",
    "# df[\"stemmed_tweets\"] = df[\"Tweet_tokenize\"].apply(lambda l: [stemmer.stem(s) for s in l])\n",
    "# df[\"stemmed_tweets\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c0d14bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# # nltk.download('wordnet')\n",
    "# # nltk.download('omw-1.4')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# df[\"lemmatize_tweets\"] = df[\"stemmed_tweets\"].apply(lambda l: [lemmatizer.lemmatize(s) for s in l])\n",
    "# df[\"lemmatize_tweets\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "64bf13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nltk.download('averaged_perceptron_tagger')\n",
    "# def pos_tag(tokens):\n",
    "#     return nltk.pos_tag(tokens)\n",
    "# df[\"postag_tweets\"] = df[\"lemmatize_tweets\"].apply(pos_tag)\n",
    "# df[\"postag_tweets\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "670f87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import ne_chunk #tokenize and POS Tagging before doing chunk\n",
    "# # !pip install svgling\n",
    "# import svgling\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# def NE_chunk(tokens):\n",
    "#     return ne_chunk(tokens)\n",
    "# df[\"nechunk_tweets\"] = df[\"postag_tweets\"].apply(NE_chunk)\n",
    "# df[\"nechunk_tweets\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "71a90585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reg = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "# chunk_grammar = r\"Chunk: {<JJ>*<NN.*>+}\"\n",
    "\n",
    "# a = nltk.RegexpParser(chunk_grammar)\n",
    "# def Chunk(tokens):\n",
    "#     a.parse(tokens)\n",
    "# df[\"chunk_tweets\"] = df[\"nechunk_tweets\"].apply(Chunk)\n",
    "# df[\"chunk_tweets\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.probability import FreqDist\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# all_tokens = []\n",
    "# for tokens in df[\"stemmed_tweets\"]:\n",
    "#     all_tokens.extend(tokens)\n",
    "# fdist = FreqDist(all_tokens)\n",
    "# print (fdist)\n",
    "# # plt.figure(figsize=(10, 5))\n",
    "# # fdist.plot(10, cumulative=False)\n",
    "# # plt.show()\n",
    "# fdist1 = fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c7d85",
   "metadata": {},
   "source": [
    "### VADER ANALYSIS (only data cleaning require:Removing link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2988d893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abhishek/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# !pip install vaderSentiment\n",
    "nltk.download('vader_lexicon')\n",
    "def sentiment_scores(sentence):\n",
    "# Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    "    else :\n",
    "        print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3620fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretty cool if you havent seen this - sketch artist on NYC subway http://nyti.ms/cvBnWk\n",
      "Overall sentiment dictionary is :  {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.6705}\n",
      "sentence was rated as  0.0 % Negative\n",
      "sentence was rated as  66.7 % Neutral\n",
      "sentence was rated as  33.300000000000004 % Positive\n",
      "Sentence Overall Rated As Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/abhishek/Desktop/PSU_Courses/CE 597 Public transportation/Project/tweets_2010.csv')\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "\n",
    "tweet_i = df[\"Tweet\"][100]\n",
    "# text_without_links = re.sub(r\"http\\S+\", \"\", tweet_i)\n",
    "\n",
    "print (tweet_i)\n",
    "sentiment_scores(tweet_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
